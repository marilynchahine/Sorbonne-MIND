{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # Week 2 Practical: Controlled Text Generation and Basic Finetuning\n",
    "#\n",
    "# In this practical, we'll explore different ways to control generation:\n",
    "# 1. **Baseline**: Free generation with different sampling strategies\n",
    "# 2. **Simple constraints**: Masking tokens to avoid specific words\n",
    "# 3. **Complex constraints**: JSON-structured output (demo)\n",
    "# 4. **Sentiment classifier finetuning**: Training a lightweight DistillBERT model\n",
    "# 5. **Semantic steering**: Biasing generation toward positive sentiment\n",
    "# 6. **Analysis**: Understanding trade-offs between control and coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5e067",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb6617",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "colab",
     "pip",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "\n",
    "%pip install accelerate==1.12.0\n",
    "%pip install bertviz==1.4.1\n",
    "%pip install datasets==4.4.1\n",
    "%pip install evaluate==0.4.6\n",
    "%pip install gensim==4.4.0\n",
    "%pip install matplotlib==3.10.7\n",
    "%pip install outlines==1.2.9\n",
    "%pip install pandas==2.3.3\n",
    "%pip install scikit-learn==1.7.2\n",
    "%pip install seaborn==0.13.2\n",
    "%pip install torch==2.9.1\n",
    "%pip install transformers==4.57.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29358e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import outlines\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList, AutoModelForSequenceClassification, pipeline, TrainingArguments, Trainer\n",
    "from typing import List, Tuple, Literal\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95487215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_device():\n",
    "    \"\"\"Returns the best device on this computer\"\"\"\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        print(f\"GPU Memory: {total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Found device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_best_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029790f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load generation model\n",
    "# We use TinyLlama-1.1B: lightweight, reliable KV caching, fits in 8GB\n",
    "# ~2.2GB in float16, leaves memory for classifier and other operations\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def build_prompt(user: str, system=\"You are a helpful assistant\"):\n",
    "    return f\"\"\"<|system|>\n",
    "{system}</s>\n",
    "<|user|>\n",
    "{user}</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40d2b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 0: Understanding Autoregressive Decoding\n",
    "\n",
    "Before using `generate()`, let's implement sampling manually how decoders work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76452f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def manual_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Manually implement autoregressive decoding to understand sampling strategies.\n",
    "\n",
    "    Decoding works by:\n",
    "    1. Encode the prompt to get initial input_ids\n",
    "    2. Forward pass: compute logits for next token\n",
    "    3. Apply sampling filters (temperature, top-k, top-p)\n",
    "    4. Sample from logits\n",
    "    5. Append sampled token to sequence\n",
    "    6. Repeat until EOS or max_length\n",
    "\n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Input text\n",
    "        max_length: Maximum generation length\n",
    "        temperature: Scales logits (higher = more random)\n",
    "        top_p: Nucleus sampling threshold (None = no limit)\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    seq_length = input_ids.shape[1]\n",
    "\n",
    "    print(f\"Starting with prompt ({seq_length} tokens): {prompt}\")\n",
    "    print(f\"Sampling strategy: temperature={temperature}, top_p={top_p}\")\n",
    "    print(f\"Generating up to {max_length} tokens...\\n\")\n",
    "\n",
    "    for step in range(max_length):\n",
    "        # Forward pass (recomputes from scratch for clarity)\n",
    "        # Production code uses KV caching via generate()\n",
    "        outputs = model(input_ids=input_ids, return_dict=True)\n",
    "\n",
    "        # Get logits for next token (last position)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # Apply top-p (nucleus) filtering\n",
    "        if top_p is not None:\n",
    "            # Implement top-p (nucleus)\n",
    "\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Compute probabilities\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Sample next token\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "        # Decode and print\n",
    "        token_str = tokenizer.decode(\n",
    "            [next_token_id.item()], skip_special_tokens=False\n",
    "        )\n",
    "        print(f\"Step {step+1}: {token_str}\", end=\" \", flush=True)\n",
    "\n",
    "        # Stop at EOS\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            print(\"\\n[EOS reached]\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\")\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different sampling strategies\n",
    "\n",
    "prompt = build_prompt(\"Write a review about a restaurant.\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLING WITH MANUAL IMPLEMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "text_manual = manual_generate(\n",
    "    model, tokenizer, prompt, max_length=15, temperature=0.7, top_p=None\n",
    ")\n",
    "print(text_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94563bc5",
   "metadata": {},
   "source": [
    "Now, implement the missing code (p-sampling)\n",
    "and play with the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLING WITH MANUAL IMPLEMENTATION (top_p=.9)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "text_manual = manual_generate(\n",
    "    model, tokenizer, prompt, max_length=15, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(text_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f8f83",
   "metadata": {},
   "source": [
    "### KV Cache Efficiency (using built-in generate)\n",
    "\n",
    "The manual loop recomputes everything. Modern transformers use KV caching\n",
    "automatically. Let's benchmark the difference using `generate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"The restaurant had\"\n",
    "max_length = 100\n",
    "\n",
    "# Warmup\n",
    "_ = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(device), max_length=10)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WITH KV CACHE (default in generate, fast)\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "for _ in range(3):\n",
    "    outputs = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\").to(device),\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "time_with_cache = (time.time() - start) / 3\n",
    "print(f\"Average time: {time_with_cache:.3f}s for {len(outputs[0])} tokens\")\n",
    "print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed917166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WITHOUT KV CACHE (use_cache=False, slow)\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "for _ in range(3):\n",
    "    outputs = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\").to(device),\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        use_cache=False,  # Disable KV caching\n",
    "    )\n",
    "time_without_cache = (time.time() - start) / 3\n",
    "print(f\"Average time: {time_without_cache:.3f}s for {len(outputs[0])} tokens\")\n",
    "print(f\"Speedup with cache: {time_without_cache / time_with_cache:.1f}x\")\n",
    "print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7c109",
   "metadata": {},
   "source": [
    "### Sampling strategies comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564adadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sampling strategies\n",
    "prompt = \"The restaurant had\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GREEDY (argmax, deterministic)\")\n",
    "print(\"=\" * 80)\n",
    "# For greedy, we need special handling (just take argmax)\n",
    "# Manual implementation is simpler with generate()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_length=40, do_sample=False, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEMPERATURE ONLY (temperature=0.7)\")\n",
    "print(\"=\" * 80)\n",
    "manual_generate(\n",
    "    model, tokenizer, prompt, max_length=15, temperature=0.7, top_p=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TOP-K SAMPLING (k=10, temperature=0.7)\")\n",
    "print(\"=\" * 80)\n",
    "manual_generate(\n",
    "    model, tokenizer, prompt, max_length=15, temperature=0.7, top_p=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c299f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TOP-P (NUCLEUS) SAMPLING (p=0.9, temperature=0.7)\")\n",
    "print(\"=\" * 80)\n",
    "manual_generate(\n",
    "    model, tokenizer, prompt, max_length=15, temperature=0.7, top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd2293",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "1. Generate text with different top-k values (k=5, 10, 20, 50). When does k\n",
    "   become too small? Too large?\n",
    "2. Generate text with different top-p values (p=0.5, 0.7, 0.9, 0.95). Compare\n",
    "   diversity vs. coherence\n",
    "3. Compare top-k vs. top-p outputs. Which feels more natural?\n",
    "4. **KV cache benchmark**: Run the speedup comparison above with different\n",
    "   max_lengths (50, 100, 200). Does speedup grow with sequence length?\n",
    "5. Try combining temperature + top-p. How do they interact? (Hint: temperature\n",
    "   smooths the distribution, top-p truncates it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a61b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 1: Baseline Generation - Playing with Sampling Strategies\n",
    "\n",
    "Let's start with free-form generation and see how different sampling\n",
    "parameters affect output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc315f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reviews(\n",
    "    prompts: List[str],\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    num_return_sequences: int = 3,\n",
    "    max_length: int = 100,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate review text with specified sampling parameters.\n",
    "\n",
    "    Args:\n",
    "        prompts: List of prompt strings\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "        top_p: Nucleus sampling threshold\n",
    "        num_return_sequences: Number of outputs per prompt\n",
    "        max_length: Maximum generation length\n",
    "\n",
    "    Returns:\n",
    "        List of generated texts\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dabde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"Write a short product review: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2923bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GREEDY DECODING (temperature=0)\")\n",
    "greedy_outputs = model.generate(\n",
    "    **tokenizer(prompt, return_tensors=\"pt\").to(device),\n",
    "    max_length=80,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(greedy_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLING (temperature=0.7, top_p=0.9)\")\n",
    "for i, text in enumerate(\n",
    "    generate_reviews(prompt, temperature=0.7, top_p=0.9, num_return_sequences=2)\n",
    "):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HIGH TEMPERATURE (temperature=1.5, top_p=0.9)\")\n",
    "for i, text in enumerate(\n",
    "    generate_reviews(prompt, temperature=1.5, top_p=0.9, num_return_sequences=2)\n",
    "):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fca915",
   "metadata": {},
   "source": [
    "**Observation**: How does increasing temperature affect diversity vs.\n",
    "coherence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b40073",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 2: Simple Constraints - Avoiding Specific Tokens\n",
    "\n",
    "Now let's implement a basic constraint: avoid generating \"yes\" or \"no\"\n",
    "directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff434809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvoidTokensLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    Mask logits for specific tokens to prevent them from being generated.\n",
    "    Useful for hard constraints like \"never say yes/no\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, tokens_to_avoid: List[str]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenizer: HuggingFace tokenizer\n",
    "            tokens_to_avoid: List of token strings to mask (e.g., [\"yes\", \"no\"])\n",
    "        \"\"\"\n",
    "        self.avoid_token_ids = set(\n",
    "            filter(None, tokenizer.convert_tokens_to_ids(tokens_to_avoid))\n",
    "        )\n",
    "        print(\n",
    "            \"AvoidTokensLogitsProcessor will filter: \"\n",
    "            + \", \".join(tokenizer.convert_ids_to_tokens(self.avoid_token_ids))\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        At each generation step, set logits of forbidden tokens to -inf.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Current sequence\n",
    "            scores: Logits for next token (batch_size, vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            Modified scores with masked tokens\n",
    "        \"\"\"\n",
    "        # Use self.avoid_token_ids to mask scores\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47474621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We use a simple \"yes/no\" inducing prompt about a camera review.\n",
    "prompt = [\n",
    "    build_prompt(\n",
    "        \"Should I buy this product?\",\n",
    "        system=\"You are discussing about a camera which has very good reviews.\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITHOUT CONSTRAINT\")\n",
    "text = generate_reviews(prompt, temperature=0.7, num_return_sequences=1)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84944ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH CONSTRAINT: avoiding 'yes' and 'no'\")\n",
    "\n",
    "# Define the constraint processor\n",
    "\n",
    "assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        logits_processor=LogitsProcessorList([constraint_processor]),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16fb80",
   "metadata": {},
   "source": [
    "**Exercise**: Modify `AvoidTokensLogitsProcessor` to also avoid patterns\n",
    "(e.g., anything starting with \"before going\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00653d",
   "metadata": {},
   "source": [
    "## Part 3: Complex Constraints - JSON Generation (Demo)\n",
    "\n",
    "For more complex structure, we'd use a library like `outlines`. Here's what it\n",
    "looks like on a very simple example – you can check structured outputs control\n",
    "by going to the [project\n",
    "repository](https://github.com/dottxt-ai/outlines?tab=readme-ov-file#customer-support-triage)\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c331d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_model = outlines.from_transformers(model, tokenizer)\n",
    "\n",
    "sentiment = outline_model(\n",
    "    build_prompt(\"Analyze: 'This product completely changed my life!'\"),\n",
    "    Literal[\"Positive\", \"Negative\", \"Neutral\"],\n",
    ")\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110975e",
   "metadata": {},
   "source": [
    "## Part 4: Semantic Steering - Sentiment-Guided Generation\n",
    "\n",
    "Now we'll finetune a sentiment classifier and use it to guide generation\n",
    "toward positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3ef67",
   "metadata": {},
   "source": [
    "### Step 1: Finetune sentiment classifier on IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8753f6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "print(\"Loading IMDb dataset...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.map(lambda x: {\"labels\": x.pop(\"label\")})\n",
    "\n",
    "# Use a subset (for speed)\n",
    "\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(10_000))\n",
    "\n",
    "# Split into train/val\n",
    "train_val = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_val[\"train\"]\n",
    "eval_dataset = train_val[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b72924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightweight sentiment model (DistillBERT)\n",
    "\n",
    "classifier_model_name = \"distilbert-base-uncased\"\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(classifier_model_name)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_model_name,\n",
    "    num_labels=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afa18b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return classifier_tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set format\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6033c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment classifier\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distillbert-imdb-finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    max_grad_norm=1.0,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=classifier_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Training sentiment classifier...\")\n",
    "classifier_model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put classifier in eval mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# And use pipeline for easy inference\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\", model=classifier_model, tokenizer=classifier_tokenizer\n",
    ")\n",
    "\n",
    "result = sentiment_classifier(\"This movie was fantastic!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = trainer.predict(eval_dataset)\n",
    "classifier_accuracy = compute_metrics(\n",
    "    (prediction_output.predictions, prediction_output.label_ids)\n",
    ")[\"accuracy\"]\n",
    "\n",
    "print(f\"Classifier accuracy on IMDb eval set: {classifier_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case training is too slow, or does not work, we can\n",
    "# skip finetuning entirely - it's unreliable on small datasets\n",
    "# Using a model trained on large balanced data is more stable\n",
    "\n",
    "if classifier_accuracy < 0.80:\n",
    "    print(\"Warning: classifier accuracy is low, loading pre-trained model...\")\n",
    "    classifier_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    classifier_tokenizer = AutoTokenizer.from_pretrained(classifier_model_name)\n",
    "    classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        classifier_model_name\n",
    "    ).to(device)\n",
    "    classifier_model.eval()\n",
    "\n",
    "    print(f\"Loaded pre-trained sentiment classifier: {classifier_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf571ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(\n",
    "        classifier_model(\n",
    "            **classifier_tokenizer(\n",
    "                [\"I like this movie a lot.\", \"Awful experience, I don't recommend.\"],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                padding=\"max_length\",\n",
    "            ).to(device)\n",
    "        ).logits.softmax(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b8cb6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 2: Implement sentiment-guided LogitsProcessor\n",
    "\n",
    "We'll implement a `LogitsProcessor` that uses the sentiment classifier. At\n",
    "each generation step, it will:\n",
    "- Score the current prefix with the sentiment classifier\n",
    "- Extract the confidence for the target sentiment (positive/negative)\n",
    "- Use this confidence to boost the LM's logits for the next token (heuristic,\n",
    "  using softmax probability multiplied by a guidance scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fefa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGuidedLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    Guide generation toward positive sentiment by scoring prefixes with a\n",
    "    sentiment classifier.\n",
    "\n",
    "    At each generation step:\n",
    "\n",
    "    1. Score the current prefix with the sentiment classifier\n",
    "    2. Compute confidence for the target sentiment (positive/negative)\n",
    "    3. Use this as a boost to the LM's logits\n",
    "\n",
    "    This is a heuristic approach (not principled like PPLM), but demonstrates\n",
    "    the core idea: classifier signal → generation signal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier,\n",
    "        tokenizer,\n",
    "        target_sentiment: int = 1,  # 0=negative, 1=positive\n",
    "        guidance_scale: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classifier: Sentiment classification model (outputs logits for [neg, pos])\n",
    "            tokenizer: Tokenizer for both LM and classifier\n",
    "            target_sentiment: Which sentiment to maximize (0 or 1)\n",
    "            guidance_scale: Strength of guidance (higher = more steering)\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_sentiment = target_sentiment\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        # [[STUDENT]]...\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eead8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Step 3: Generate with sentiment guidance\n",
    "\n",
    "Now we can use the sentiment-guided processor during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: unguided vs. guided generation\n",
    "prompt = build_prompt(\"Please write a review about the MasterMind camera\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"UNGUIDED GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "unguided_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i, text in enumerate(unguided_texts):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ede44f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GUIDED GENERATION (target: positive sentiment)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "sentiment_processor = SentimentGuidedLogitsProcessor(\n",
    "    classifier=classifier_model,\n",
    "    tokenizer=classifier_tokenizer,\n",
    "    target_sentiment=1,  # positive\n",
    "    guidance_scale=0.5,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2,\n",
    "        logits_processor=LogitsProcessorList([sentiment_processor]),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "pos_guided_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i, text in enumerate(pos_guided_texts):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GUIDED GENERATION (target: negative sentiment)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2,\n",
    "        logits_processor=LogitsProcessorList([sentiment_processor]),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "sentiment_processor = SentimentGuidedLogitsProcessor(\n",
    "    classifier=classifier_model,\n",
    "    tokenizer=classifier_tokenizer,\n",
    "    target_sentiment=0,  # positive\n",
    "    guidance_scale=0.5,\n",
    ")\n",
    "\n",
    "neg_guided_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i, text in enumerate(neg_guided_texts):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619538",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 5: Analysis - Measuring Guidance Effectiveness\n",
    "\n",
    "In this section, we'll use the sentiment classifier to quantitatively evaluate\n",
    "how well the guidance worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b249f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(texts: List[str]) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Score texts with the sentiment classifier.\n",
    "    Returns positive and negative probabilities.\n",
    "    \"\"\"\n",
    "    inputs = classifier_tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = classifier_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    return probs[:, 0].cpu().numpy(), probs[:, 1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentiment scores\n",
    "print(\"=\" * 80)\n",
    "print(\"SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "neg_probs_unguidedled, pos_probs_unguided = score_sentiment(unguided_texts)\n",
    "neg_probs_pos_guided, pos_probs_pos_guided = score_sentiment(pos_guided_texts)\n",
    "neg_probs_neg_guided, pos_probs_neg_guided = score_sentiment(neg_guided_texts)\n",
    "\n",
    "print(\"\\nUNGUIDED:\")\n",
    "for i, (text, pos_prob) in enumerate(zip(unguided_texts, pos_probs_unguided)):\n",
    "    print(f\"  Sample {i+1}: {pos_prob:.2%} positive\")\n",
    "\n",
    "print(\"\\nGUIDED (target: positive):\")\n",
    "for i, (text, pos_prob) in enumerate(zip(pos_guided_texts, pos_probs_pos_guided)):\n",
    "    print(f\"  Sample {i+1}: {pos_prob:.2%} positive\")\n",
    "\n",
    "print(\"\\nGUIDED (target: negative):\")\n",
    "for i, (text, pos_prob) in enumerate(zip(neg_guided_texts, pos_probs_neg_guided)):\n",
    "    print(f\"  Sample {i+1}: {pos_prob:.2%} positive\")\n",
    "\n",
    "print(\"\\nAverage positive probability:\")\n",
    "print(f\"  Unguided: {pos_probs_unguided.mean():.2%}\")\n",
    "print(f\"  Guided (pos):   {pos_probs_pos_guided.mean():.2%}\")\n",
    "print(f\"  Guided (neg):   {pos_probs_neg_guided.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d3acc",
   "metadata": {},
   "source": [
    "## Part 6: Exploration - Varying Guidance Strength\n",
    "\n",
    "- How does `guidance_scale` affect the trade-off between sentiment control and\n",
    "  fluency?\n",
    "- Can you train a better classifier to improve guidance (e.g. FUDGE) and\n",
    "  modify the `SentimentGuidedLogitsProcessor` accordingly?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
