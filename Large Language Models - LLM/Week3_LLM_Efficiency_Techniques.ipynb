{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8698d16a",
   "metadata": {},
   "source": [
    "# Week 3 Practical: LLM Efficiency Techniques\n",
    "\n",
    "## Training and Inference Optimization\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this practical session, you will:\n",
    "\n",
    "1. **Implement LoRA from scratch** (Exercise 1)\n",
    "   - Understand low-rank adaptation mechanics (W' = W + BA)\n",
    "   - Apply LoRA to DistilBERT attention layers\n",
    "   - Understand how to patch a neural networks (torch)\n",
    "   - Compare to full fine-tuning (~0.5% parameters, similar accuracy)\n",
    "\n",
    "2. **Implement INT8 Quantization** (Exercise 2)\n",
    "   - Quantize FFN layers manually using symmetric quantization\n",
    "   - Analyze accuracy/memory trade-offs (4x compression)\n",
    "   - Understand post-training quantization techniques\n",
    "\n",
    "3. **Use HuggingFace PEFT Library** (Exercise 3)\n",
    "   - Experiment with production PEFT methods (LoRA, Adapters, Prefix Tuning)\n",
    "   - Compare different parameter-efficient fine-tuning approaches\n",
    "   - Learn configuration patterns for real-world use\n",
    "\n",
    "4. **Benchmark Performance** (Exercise 4)\n",
    "   - Measure memory usage, training time, inference speed\n",
    "   - Compare all efficiency techniques systematically\n",
    "   - Understand real-world trade-offs for deployment decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf46bf7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The next cell imports the necessary modules â€“ just run it, no need to look at\n",
    "the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c47e5e1-5829-4f72-97d1-7176a5c014a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b369365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 16:35:22.714651: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 16:35:22.832038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 16:35:25.092966: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from typing import Tuple, List\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa754863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 6.4 GB\n",
      "GPU Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Found device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_best_device():\n",
    "    \"\"\"Returns the best device on this computer\"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        print(f\"GPU Memory: {total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Found device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232e06b",
   "metadata": {},
   "source": [
    "### Load IMDB Dataset\n",
    "\n",
    "We'll use a subset of IMDB for faster experimentation with efficiency techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d5d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDb dataset...\n",
      "Train: 9500, Eval: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDb dataset...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.map(lambda x: {\"labels\": x.pop(\"label\")})\n",
    "\n",
    "# Use a subset (for speed in efficiency experiments)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Default dataset configuration (can be modified for faster experimentation)\n",
    "dataset_size = 10_000\n",
    "eval_split = 0.05\n",
    "\n",
    "\n",
    "dataset = dataset.select(range(dataset_size))\n",
    "\n",
    "# Split into train/val\n",
    "train_val = dataset.train_test_split(test_size=eval_split, seed=42)\n",
    "train_dataset = train_val[\"train\"]\n",
    "eval_dataset = train_val[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea4bda",
   "metadata": {},
   "source": [
    "### Load Pre-trained Sentiment Classifier\n",
    "\n",
    "Instead of training from scratch, we'll use a pre-trained DistilBERT model\n",
    "fine-tuned on SST-2 (Stanford Sentiment Treebank). This saves time and provides\n",
    "a strong baseline for our efficiency experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aca97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained sentiment classifier...\n",
      "Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "Parameters: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pre-trained sentiment classifier...\")\n",
    "classifier_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(classifier_model_name)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_model_name\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: {classifier_model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in classifier_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1614656",
   "metadata": {},
   "source": [
    "### Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26009ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a23d1486964dcb84a3fda911af2f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets tokenized with 256 tokens per example\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return classifier_tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\n",
    "    f\"Datasets tokenized with {len(train_dataset[0]['input_ids'])} tokens per example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48dd84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline accuracy...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on IMDB: 0.8960\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline accuracy on IMDB dataset\n",
    "print(\"Evaluating baseline accuracy...\")\n",
    "classifier_model.eval()\n",
    "\n",
    "# Compute baseline accuracy\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained-models/tmp\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=classifier_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "prediction_output = trainer.predict(eval_dataset)\n",
    "classifier_accuracy = compute_metrics(\n",
    "    (prediction_output.predictions, prediction_output.label_ids)\n",
    ")[\"accuracy\"]\n",
    "\n",
    "print(f\"Baseline accuracy on IMDB: {classifier_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc402e5",
   "metadata": {},
   "source": [
    "# IMDB statistics and baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fee927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train samples: 9500\n",
      "Test samples: 500\n",
      "Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "Baseline accuracy: 0.8960\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {device}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(eval_dataset)}\")\n",
    "print(f\"Model: {classifier_model_name}\")\n",
    "print(f\"Baseline accuracy: {classifier_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b893d1",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 1: Manual LoRA Implementation\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method\n",
    "that freezes the pre-trained model weights and injects trainable low-rank\n",
    "matrices into each layer.\n",
    "\n",
    "**Key Idea:** Instead of updating $W \\in \\mathbb{R}^{d \\times d}$ directly, we\n",
    "learn:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "where:\n",
    "- $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times d}$\n",
    "- $r \\ll d$ (rank, typically 4-16)\n",
    "- $W$ is frozen, only $A$ and $B$ are trained\n",
    "\n",
    "**Paper:** [LoRA: Low-Rank Adaptation of Large Language\n",
    "Models](https://arxiv.org/abs/2106.09685) (Hu et al., 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661a4ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 1.1: Implement LoRALayer\n",
    "\n",
    "We'll implement a LoRA layer that wraps a standard PyTorch Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca13cf86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) layer.\n",
    "\n",
    "    Implements: output = W @ x + (B @ A) @ x * (alpha / r)\n",
    "    where W is frozen, A and B are trainable.\n",
    "\n",
    "    Args:\n",
    "        original_layer: The linear layer to adapt\n",
    "        r: Rank of adaptation matrices (default: 8)\n",
    "        alpha: Scaling factor (default: 16, effective scale is alpha/r)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        r: int = 8,\n",
    "        alpha: float = 16,\n",
    "    ):\n",
    "        super(LoRALayer, self).__init__()\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "        \n",
    "        # Initialize LoRA matrices A and B\n",
    "        \n",
    "        # Initialize A with kaiming_uniform (standard init)\n",
    "        self.A = torch.nn.Linear(original_layer.in_features, r)\n",
    "        torch.nn.init.kaiming_uniform_(self.A.weight)\n",
    "        \n",
    "        # Initialize B with zeros (important: starts as identity)\n",
    "        self.B = torch.nn.Linear(original_layer.in_features, r)\n",
    "        torch.nn.init.zeros_(self.B.weight)\n",
    "        \n",
    "        # Freeze original layer\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Implement LoRA forward pass\n",
    "\n",
    "        # Compute original output\n",
    "        W =  self.original_layer @ x\n",
    "        \n",
    "        # Compute LoRA adaptation: B @ A @ x, scaled by alpha/r\n",
    "        LoRA_adapt = (self.B @ self.A) @ x * (alpha / r)\n",
    "        \n",
    "        return W + LoRA_adapt\n",
    "\n",
    "\n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into original layer for deployment.\"\"\"\n",
    "        # Optional: Implement weight merging\n",
    "\n",
    "        assert False, 'Not implemented yet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a345432",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 1.2: Helper Functions\n",
    "\n",
    "These helper functions will apply LoRA to DistilBERT's attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47d436d7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def apply_lora_to_distilbert(\n",
    "    model: nn.Module,\n",
    "    r: int = 8,\n",
    "    alpha: float = 16,\n",
    "    target_modules: List[str] = None,\n",
    ") -> List[nn.Parameter]:\n",
    "    \"\"\"\n",
    "    Apply LoRA to specific modules in DistilBERT.\n",
    "\n",
    "    Args:\n",
    "        model: DistilBERT model\n",
    "        r: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "        target_modules: Which attention matrices to adapt (q_lin, k_lin, v_lin, out_lin)\n",
    "\n",
    "    Returns:\n",
    "        List of LoRA parameters to optimize\n",
    "    \"\"\"\n",
    "    if target_modules is None:\n",
    "        target_modules = [\"q_lin\", \"v_lin\"]\n",
    "\n",
    "    lora_params = []\n",
    "\n",
    "    # Iterate through transformer layers\n",
    "    for i, layer in tqdm(enumerate(model.distilbert.transformer.layer)):\n",
    "        attention = layer.attention\n",
    "\n",
    "        for module_name in target_modules:\n",
    "            if hasattr(attention, module_name):\n",
    "                original_layer = getattr(attention, module_name)\n",
    "\n",
    "                # Replace with LoRA layer\n",
    "                lora_layer = LoRALayer(original_layer, r=r, alpha=alpha)\n",
    "                setattr(attention, module_name, lora_layer)\n",
    "\n",
    "                # Collect LoRA parameters\n",
    "                lora_params.extend([lora_layer.A.weight, lora_layer.B.weight])\n",
    "\n",
    "                print(f\"Applied LoRA to layer {i} -> {module_name}\")\n",
    "\n",
    "    return lora_params\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model: nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Count trainable vs total parameters.\n",
    "\n",
    "    Returns:\n",
    "        (trainable_params, total_params)\n",
    "    \"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bd27a523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_model_name,\n",
    "    num_labels=2,\n",
    ").to(device)\n",
    "\n",
    "# And look at its parameters\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860b6b1",
   "metadata": {},
   "source": [
    "## Step 1.3: Apply LoRA and Train\n",
    "\n",
    "Now let's apply LoRA to a DistilBERT model and fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a43e62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 339.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied LoRA to layer 0 -> q_lin\n",
      "Applied LoRA to layer 0 -> v_lin\n",
      "Applied LoRA to layer 1 -> q_lin\n",
      "Applied LoRA to layer 1 -> v_lin\n",
      "Applied LoRA to layer 2 -> q_lin\n",
      "Applied LoRA to layer 2 -> v_lin\n",
      "Applied LoRA to layer 3 -> q_lin\n",
      "Applied LoRA to layer 3 -> v_lin\n",
      "Applied LoRA to layer 4 -> q_lin\n",
      "Applied LoRA to layer 4 -> v_lin\n",
      "Applied LoRA to layer 5 -> q_lin\n",
      "Applied LoRA to layer 5 -> v_lin\n",
      "Ratio of parameters trained:  147648 67102658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup LoRA training\n",
    "\n",
    "# Freeze ALL model parameters first\n",
    "for param in lora_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Apply LoRA to attention layers (q_lin and v_lin)\n",
    "apply_lora_to_distilbert(lora_model)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable, total = count_trainable_parameters(lora_model)\n",
    "print(\"Ratio of parameters trained: \", trainable, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7122a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LoRA model (2 epochs)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'Linear' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     10\u001b[39m training_args = TrainingArguments(\n\u001b[32m     11\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./trained-models/lora-distillbert-imdb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     num_train_epochs=num_epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     report_to=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m trainer = Trainer(\n\u001b[32m     25\u001b[39m     model=lora_model,\n\u001b[32m     26\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     35\u001b[39m lora_results = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:905\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    903\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    915\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:466\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m \u001b[33;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m sa_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    475\u001b[39m     sa_output, sa_weights = sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:388\u001b[39m, in \u001b[36mDistilBertSdpaAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"group heads\"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads * dim_per_head)\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m q = shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[32m    389\u001b[39m k = shape(\u001b[38;5;28mself\u001b[39m.k_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m    390\u001b[39m v = shape(\u001b[38;5;28mself\u001b[39m.v_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mind/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mLoRALayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Implement LoRA forward pass\u001b[39;00m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# Compute original output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     W =  \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moriginal_layer\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Compute LoRA adaptation: B @ A @ x, scaled by alpha/r\u001b[39;00m\n\u001b[32m     48\u001b[39m     LoRA_adapt = (\u001b[38;5;28mself\u001b[39m.B @ \u001b[38;5;28mself\u001b[39m.A) @ x * (alpha / r)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for @: 'Linear' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Train LoRA model\n",
    "\n",
    "# Default training configuration (can be reduced for faster experimentation)\n",
    "num_epochs = 2\n",
    "logging_steps = 100\n",
    "\n",
    "\n",
    "print(f\"\\nTraining LoRA model ({num_epochs} epochs)...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained-models/lora-distillbert-imdb\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "lora_results = trainer.evaluate()\n",
    "print(f\"\\nLoRA model accuracy: {lora_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f759fe9",
   "metadata": {},
   "source": [
    "## Step 1.4: Compare to Baseline\n",
    "\n",
    "Let's compare LoRA to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: LoRA vs Full Fine-tuning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full fine-tuning baseline (from distillbert_imdb.py)\n",
    "baseline_accuracy = classifier_accuracy\n",
    "print(f\"Full fine-tuning: {baseline_accuracy:.4f} accuracy, 66M trainable params\")\n",
    "print(\n",
    "    f\"LoRA (r=8):       {lora_results['eval_accuracy']:.4f} accuracy, \"\n",
    "    f\"{trainable:,} trainable params\"\n",
    ")\n",
    "print(f\"\\nParameter reduction: {100*(1-trainable/66e6):.2f}%\")\n",
    "print(f\"Accuracy drop: {100*(baseline_accuracy - lora_results['eval_accuracy']):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56196fbf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Parameters comparison\n",
    "methods = [\"Full FT\", \"LoRA (r=8)\"]\n",
    "params = [66e6, trainable]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "\n",
    "ax1.bar(methods, params, color=colors)\n",
    "ax1.set_ylabel(\"Trainable Parameters\")\n",
    "ax1.set_title(\"Parameter Efficiency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "for i, (method, param) in enumerate(zip(methods, params)):\n",
    "    ax1.text(i, param, f\"{param/1e6:.1f}M\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracies = [baseline_accuracy, lora_results[\"eval_accuracy\"]]\n",
    "ax2.bar(methods, accuracies, color=colors)\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_title(\"Model Accuracy\")\n",
    "ax2.set_ylim([0.80, 0.95])\n",
    "for i, (method, acc) in enumerate(zip(methods, accuracies)):\n",
    "    ax2.text(i, acc, f\"{acc:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97ccfb",
   "metadata": {},
   "source": [
    "## Exercise 1 Extension: Experiment with Rank\n",
    "\n",
    "Try different LoRA ranks and see the trade-off between parameters and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f764b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different ranks\n",
    "\n",
    "    # Apply LoRA\n",
    "    \n",
    "    # Count params\n",
    "    \n",
    "    # Train (1 epoch for speed)\n",
    "    \n",
    "    # Evaluate\n",
    "    \n",
    "# Plot results\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9c76c",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2: Manual INT8 Quantization\n",
    "\n",
    "## What is Quantization?\n",
    "\n",
    "**Quantization** converts high-precision floating-point weights (FP32: 32\n",
    "bits) to low-precision integers (INT8: 8 bits).\n",
    "\n",
    "**Symmetric Quantization Formula:**\n",
    "\n",
    "$$\\text{quantized} =\n",
    "\\text{round}\\left(\\frac{\\text{original}}{\\text{scale}}\\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\text{scale} = \\frac{\\max(|\\text{original}|)}{127}$$\n",
    "\n",
    "**Dequantization:**\n",
    "\n",
    "$$\\text{dequantized} = \\text{quantized} \\times \\text{scale}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8719d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 2.1: Implement Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(\n",
    "    tensor: torch.Tensor,\n",
    "    bits: int = 8,\n",
    ") -> Tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Quantize tensor to INT8 using symmetric quantization.\n",
    "\n",
    "    Args:\n",
    "        tensor: FP32 tensor to quantize\n",
    "        bits: Number of bits (default: 8)\n",
    "\n",
    "    Returns:\n",
    "        quantized: INT8 tensor\n",
    "        scale: Scale factor for dequantization\n",
    "    \"\"\"\n",
    "    # Implement symmetric INT8 quantization\n",
    "\n",
    "    # Compute scale factor\n",
    "    \n",
    "    # Quantize: divide by scale, round, clamp to INT8 range\n",
    "    \n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "def dequantize_tensor(\n",
    "    quantized: torch.Tensor,\n",
    "    scale: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dequantize INT8 tensor back to FP32.\n",
    "\n",
    "    Args:\n",
    "        quantized: INT8 tensor\n",
    "        scale: Scale factor from quantization\n",
    "\n",
    "    Returns:\n",
    "        dequantized: FP32 tensor\n",
    "    \"\"\"\n",
    "    # Implement dequantization\n",
    "\n",
    "    # Convert to float and multiply by scale\n",
    "    \n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df075e8",
   "metadata": {},
   "source": [
    "## Step 2.2: Test Quantization\n",
    "\n",
    "Let's test our quantization on a sample tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quantization\n",
    "test_tensor = torch.randn(1000, 768)\n",
    "print(f\"Original tensor: shape={test_tensor.shape}, dtype={test_tensor.dtype}\")\n",
    "print(f\"Original range: [{test_tensor.min():.4f}, {test_tensor.max():.4f}]\")\n",
    "print(f\"Original memory: {test_tensor.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "# Quantize\n",
    "quantized, scale = quantize_tensor(test_tensor)\n",
    "print(f\"\\nQuantized tensor: shape={quantized.shape}, dtype={quantized.dtype}\")\n",
    "print(f\"Quantized range: [{quantized.min()}, {quantized.max()}]\")\n",
    "print(f\"Quantized memory: {quantized.nbytes / 1e6:.2f} MB\")\n",
    "print(f\"Scale factor: {scale:.6f}\")\n",
    "print(f\"Memory reduction: {test_tensor.nbytes / quantized.nbytes:.1f}x\")\n",
    "\n",
    "# Dequantize\n",
    "dequantized = dequantize_tensor(quantized, scale)\n",
    "print(f\"\\nDequantized tensor: shape={dequantized.shape}, dtype={dequantized.dtype}\")\n",
    "print(f\"Dequantized range: [{dequantized.min():.4f}, {dequantized.max():.4f}]\")\n",
    "\n",
    "# Measure error\n",
    "mse = ((test_tensor - dequantized) ** 2).mean()\n",
    "print(f\"\\nReconstruction MSE: {mse:.6f}\")\n",
    "print(f\"Relative error: {(mse / test_tensor.var()).sqrt():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed32499",
   "metadata": {},
   "source": [
    "## Step 2.3: Visualize Quantization Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(test_tensor.flatten().numpy(), bins=100, alpha=0.7, color=\"blue\")\n",
    "axes[0].set_title(\"Original (FP32)\")\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Quantized\n",
    "axes[1].hist(quantized.flatten().numpy(), bins=50, alpha=0.7, color=\"orange\")\n",
    "axes[1].set_title(\"Quantized (INT8)\")\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "\n",
    "# Dequantized vs Original\n",
    "axes[2].scatter(\n",
    "    test_tensor.flatten().numpy()[::100],\n",
    "    dequantized.flatten().numpy()[::100],\n",
    "    alpha=0.3,\n",
    "    s=1,\n",
    ")\n",
    "axes[2].plot([-4, 4], [-4, 4], \"r--\", label=\"Perfect reconstruction\")\n",
    "axes[2].set_title(\"Dequantized vs Original\")\n",
    "axes[2].set_xlabel(\"Original\")\n",
    "axes[2].set_ylabel(\"Dequantized\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd162a88",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 2.4: Implement Quantized Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e667b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantized linear layer for FFN.\n",
    "\n",
    "    Stores weights in INT8, dequantizes during forward pass.\n",
    "    This is post-training quantization (PTQ).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        # Initialize quantized linear layer\n",
    "\n",
    "        # Quantize weight matrix\n",
    "        \n",
    "        # Store as buffer (not parameter - don't train)\n",
    "        \n",
    "        # Keep bias as float (small, not worth quantizing)\n",
    "        \n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Forward pass with dequantization\n",
    "\n",
    "        # Dequantize weights on the fly\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        \n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    def get_memory_size(self) -> float:\n",
    "        \"\"\"Returns memory size in MB.\"\"\"\n",
    "        size = self.weight_quantized.nbytes  # INT8 weights\n",
    "        if self.bias is not None:\n",
    "            size += self.bias.nbytes\n",
    "        return size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7791a08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 2.5: Quantize DistilBERT FFN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed0228",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def quantize_distilbert_ffn(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Quantize all FFN layers (lin1, lin2) in DistilBERT.\n",
    "\n",
    "    Args:\n",
    "        model: DistilBERT model\n",
    "\n",
    "    Returns:\n",
    "        Model with quantized FFN layers\n",
    "    \"\"\"\n",
    "    for i, layer in enumerate(model.distilbert.transformer.layer):\n",
    "        ffn = layer.ffn\n",
    "\n",
    "        # Quantize lin1 (768 -> 3072)\n",
    "        ffn.lin1 = QuantizedLinear(ffn.lin1)\n",
    "\n",
    "        # Quantize lin2 (3072 -> 768)\n",
    "        ffn.lin2 = QuantizedLinear(ffn.lin2)\n",
    "\n",
    "        print(f\"Quantized layer {i} FFN (lin1, lin2)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def measure_model_size(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Measure model size in MB.\n",
    "\n",
    "    Returns:\n",
    "        Size in MB\n",
    "    \"\"\"\n",
    "    size = 0\n",
    "    for param in model.parameters():\n",
    "        size += param.numel() * param.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        size += buffer.numel() * buffer.element_size()\n",
    "    return size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b71f82",
   "metadata": {},
   "source": [
    "## Step 2.6: Quantize and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model and analyze results\n",
    "\n",
    "# Measure original size\n",
    "# Quantize FFN layers\n",
    "\n",
    "# Measure quantized size\n",
    "assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "# Evaluate accuracy\n",
    "quantized_model.eval()\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer_quant = Trainer(\n",
    "    model=quantized_model,\n",
    "    args=TrainingArguments(output_dir=\"./trained-models/tmp\", report_to=None),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "prediction_output = trainer_quant.predict(eval_dataset)\n",
    "quantized_accuracy = compute_metrics(\n",
    "    (prediction_output.predictions, prediction_output.label_ids)\n",
    ")[\"accuracy\"]\n",
    "\n",
    "print(f\"\\nOriginal accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"Quantized accuracy: {quantized_accuracy:.4f}\")\n",
    "print(f\"Accuracy drop: {100*(baseline_accuracy - quantized_accuracy):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Memory comparison\n",
    "methods = [\"FP32\", \"INT8\"]\n",
    "sizes = [size_before, size_after]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "\n",
    "ax1.bar(methods, sizes, color=colors)\n",
    "ax1.set_ylabel(\"Model Size (MB)\")\n",
    "ax1.set_title(\"Memory Efficiency\")\n",
    "for i, (method, size) in enumerate(zip(methods, sizes)):\n",
    "    ax1.text(i, size, f\"{size:.1f} MB\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracies = [baseline_accuracy, quantized_accuracy]\n",
    "ax2.bar(methods, accuracies, color=colors)\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_title(\"Model Accuracy\")\n",
    "ax2.set_ylim([0.80, 0.95])\n",
    "for i, (method, acc) in enumerate(zip(methods, accuracies)):\n",
    "    ax2.text(i, acc, f\"{acc:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1d8e",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 3: HuggingFace PEFT Library\n",
    "\n",
    "## Introduction to PEFT\n",
    "\n",
    "The [PEFT (Parameter-Efficient Fine-Tuning) library by\n",
    "HuggingFace](https://huggingface.co/blog/peft) provides production-ready\n",
    "implementations of various efficiency methods:\n",
    "\n",
    "- **LoRA**: Low-rank adaptation (we implemented this in Exercise 1)\n",
    "- **Adapters**: Bottleneck layers inserted after transformer blocks\n",
    "- **Prefix Tuning**: Learnable \"virtual tokens\" prepended to input\n",
    "- **IA3**: Learned element-wise rescaling of activations\n",
    "\n",
    "**Benefits:**\n",
    "- Battle-tested implementations\n",
    "- Easy to switch between methods\n",
    "- Modular: One base model, many task-specific adapters\n",
    "- Active development and community support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bbc2b",
   "metadata": {
    "incorrectly_encoded_metadata": "tags="
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    IA3Config,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddbece",
   "metadata": {},
   "source": [
    "## Step 3.2: LoRA with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA using PEFT library\n",
    "\n",
    "# Configure LoRA\n",
    "\n",
    "# Apply LoRA\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff90313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PEFT LoRA model\n",
    "training_args_peft = TrainingArguments(\n",
    "    output_dir=\"./trained-models/peft-lora-distillbert-imdb\",\n",
    "    num_train_epochs=num_epochs,  # Use same config as manual LoRA\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,  # Use same config as manual LoRA\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_lora_model,\n",
    "    args=training_args_peft,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_peft.train()\n",
    "peft_lora_results = trainer_peft.evaluate()\n",
    "print(f\"PEFT LoRA accuracy: {peft_lora_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b06af3",
   "metadata": {},
   "source": [
    "## Step 3.3: Compare Other PEFT Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3956922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different PEFT methods\n",
    "\n",
    "# Define configurations\n",
    "\n",
    "    # Load fresh base model\n",
    "    \n",
    "    # Apply PEFT\n",
    "    \n",
    "    # Train\n",
    "    \n",
    "    # Evaluate\n",
    "    \n",
    "# Display results\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f33502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Trainable parameters\n",
    "methods = df_peft[\"Method\"].tolist()\n",
    "params = df_peft[\"Trainable Params\"].tolist()\n",
    "colors = plt.cm.Set3(range(len(methods)))\n",
    "\n",
    "ax1.barh(methods, params, color=colors)\n",
    "ax1.set_xlabel(\"Trainable Parameters\")\n",
    "ax1.set_title(\"Parameter Efficiency\")\n",
    "ax1.set_xscale(\"log\")\n",
    "for i, (method, param) in enumerate(zip(methods, params)):\n",
    "    ax1.text(param, i, f\"  {param/1e6:.2f}M\", va=\"center\")\n",
    "\n",
    "# Accuracy\n",
    "accuracies = df_peft[\"Accuracy\"].tolist()\n",
    "ax2.barh(methods, accuracies, color=colors)\n",
    "ax2.set_xlabel(\"Accuracy\")\n",
    "ax2.set_title(\"Model Accuracy\")\n",
    "ax2.set_xlim([0.80, 0.95])\n",
    "for i, (method, acc) in enumerate(zip(methods, accuracies)):\n",
    "    ax2.text(acc, i, f\"  {acc:.3f}\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727f288",
   "metadata": {},
   "source": [
    "## Step 3.4: Save and Load PEFT Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load PEFT adapter\n",
    "\n",
    "# Save adapter (only the adapter weights, not the base model)\n",
    "\n",
    "# Load adapter back\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1af8a",
   "metadata": {},
   "source": [
    "---\n",
    "# Project: Performance Benchmarking\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that we've explored various efficiency techniques, you can\n",
    "study their effect on:\n",
    "\n",
    "- **Memory**: Model size (MB)\n",
    "- **Training Time**: Time per epoch (seconds)\n",
    "- **Inference Speed**: Samples per second\n",
    "- **Accuracy**: Test set accuracy\n",
    "\n",
    "This will help us understand real-world trade-offs and make informed\n",
    "decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
